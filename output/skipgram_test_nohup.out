nohup: ignoring input

=================== MODEL PARAMETERS: =================== 

general_data_dir:                /home/diegor/data/
data_dir:                        data/
word_embeddings_dir:             /home/diegor/data/word_embeddings/
vocab_cutoff:                    5
w2v_path:                        None
data_augmentation_ratio:         0.25
w2v_init:                        False
syn_augm:                        True
split_ratio:                     0.9
embedding_size:                  300
epochs:                          1
batch_size:                      10
ctx_size:                        5
num_neg_samples:                 5
learning_rate:                   0.01
model_name:                      rand_init-syns-25r-1e-voc5-emb300
all_models_dir:                  model/
model_dir:                       model/rand_init-syns-25r-1e-voc5-emb300/
model_file:                      /home/diegor/knowledge-augmented-skipgram/model/rand_init-syns-25r-1e-voc5-emb300/rand_init-syns-25r-1e-voc5-emb300.pth
checkpoints_dir:                 /home/diegor/knowledge-augmented-skipgram/model/rand_init-syns-25r-1e-voc5-emb300/checkpoints/
input_emb_file:                  /home/diegor/knowledge-augmented-skipgram/model/rand_init-syns-25r-1e-voc5-emb300/rand_init-syns-25r-1e-voc5-emb300
bnc_data_dir:                    /home/diegor/data/British_National_Corpus/bnc_full_processed_data/
bnc_data:                        /home/diegor/data/British_National_Corpus/bnc_full_processed_data/bnc_full_proc_data.txt
bnc_tags:                        /home/diegor/data/British_National_Corpus/bnc_full_processed_data/bnc_full_proc_data_tags.txt
use_data_subset:                 True
data_subset_size:                0.5
bnc_subset_data:                 /home/diegor/data/British_National_Corpus/bnc_full_processed_data/bnc_full_proc_data_shffl_sub-5.txt
bnc_subset_tags:                 /home/diegor/data/British_National_Corpus/bnc_full_processed_data/bnc_full_proc_data_shffl_sub-5_tags.txt
train_data:                      data/bnc_full_proc_data_shffl_sub-5_train.txt
train_tags:                      data/bnc_full_proc_data_shffl_sub-5_train_tags.txt
val_data:                        data/bnc_full_proc_data_shffl_sub-5_val.txt
val_tags:                        data/bnc_full_proc_data_shffl_sub-5_val_tags.txt
train_skipgram_data:             data/skipgram_bnc_full_proc_data_shffl_sub-5_train.csv
train_skipgram_augm_data:        data/skipgram_augm_bnc_full_proc_data_shffl_sub-5_train.csv
val_skipgram_data:               data/skipgram_bnc_full_proc_data_shffl_sub-5_val.csv
val_skipgram_augm_data:          data/skipgram_augm_bnc_full_proc_data_shffl_sub-5_val.csv
train_skipgram_sampled_data:     data/sampled_skipgram_bnc_full_proc_data_shffl_sub-5_train.csv
train_skipgram_augm_sampled_data:  data/sampled_skipgram_augm_bnc_full_proc_data_shffl_sub-5_train.csv
val_skipgram_sampled_data:       data/sampled_skipgram_bnc_full_proc_data_shffl_sub-5_val.csv
val_skipgram_augm_sampled_data:  data/sampled_skipgram_augm_bnc_full_proc_data_shffl_sub-5_val.csv
synonym_selection:               sw
train_skipgram_syns_data:        data/syns-sw_sampled_skipgram_bnc_full_proc_data_shffl_sub-5_train.csv
val_skipgram_syns_data:          data/syns-sw_sampled_skipgram_bnc_full_proc_data_shffl_sub-5_val.csv
num_train_skipgram_data:         data/num_voc-5_skipgram_bnc_full_proc_data_shffl_sub-5_train.csv
num_train_skipgram_syns_data:    data/num_voc-5_syns-sw_sampled_skipgram_bnc_full_proc_data_shffl_sub-5_train.csv
num_val_skipgram_data:           data/num_voc-5_skipgram_bnc_full_proc_data_shffl_sub-5_val.csv
num_val_skipgram_syns_data:      data/num_voc-5_syns-sw_sampled_skipgram_bnc_full_proc_data_shffl_sub-5_val.csv
counts_file:                     /home/diegor/data/British_National_Corpus/bnc_full_processed_data/counts_bnc_full_proc_data_shffl_sub-5.csv

=================== / MODEL PARAMETERS: =================== 

Running on device: cuda
Size of sample table:  100016576
Total distinct words:  373560
Samples from vocab:  [['prefers', '193', '5.157286137698524e-06'], ['to', '990207', '0.02646000432410385'], ['remain', '3361', '8.98115995274857e-05'], ['in', '738390', '0.01973102855551924'], ['her', '140006', '0.0037411969067078737']]
Epoch 0 validation
29799980

********
Validation loss at epoch 0: 640.88
(num points: 29799980 )
Calculating synonym indices. Number of synonyms: 112200002
Finished calculating. Number of random synonym indices: 336600006
Traceback (most recent call last):
  File "train_skipgram.py", line 351, in <module>
    focus_i = int(row[0])
ValueError: invalid literal for int() with base 10: 'focus_word'
