# Losses plot Skip-Gram experiments

import matplotlib.pyplot as plt

# Losses per epoch in word2vec initialised augmented training
# Train losses:
w2v_losses = [90.99192615362155, 90.99192615362155, 39.61688507546849, 38.52134469891212, 37.23085300652096, 37.52140476959677, 36.40775369531093, 36.361427991396596, 35.60928881980919, 35.242723151701455, 35.64702307499139]

# Validation losses:
w2v_val_losses = [83.69201210960793, 37.85567452431703, 37.03749130281308, 36.24992709750306, 35.99865646843791, 35.93882035610234, 35.646460382388526, 35.67885671829581, 35.93830122688148, 35.60192754425153, 35.15087626853718]

# Losses per epoch in randomly initialised augmented training
# Train losses:
rand_losses = [466.40729121250763, 466.40729121250763, 52.01002995899468, 45.33652249498692, 40.82703263494943, 41.246863034954806, 37.81643397722055, 38.258799070996815, 36.763186337522306, 36.416434987675146, 37.244609392571434]
# Validation losses:
rand_val_losses = [514.0520100623453, 53.06532562840136, 48.229698228714774, 45.45609444968102, 43.537568774081834, 41.30135674243997, 40.11106407421368, 40.291240622425825, 40.12016293203536, 38.93582209018828, 38.66495394997641]

# Losses per epoch in word2vec initialised non-augmented training
# Train losses:
non_augm_losses = [89.22938830495703, 89.22938830495703, 42.22861029358843, 40.68845572273082, 39.79860587405458, 39.47102180587483, 39.15461915474952, 38.27463470180621, 38.411626062547015, 37.76083002341855, 37.528871906983774]
# Validation losses:
non_augm_val_losses = [83.69201210960793, 38.48004039559523, 37.88853268377202, 36.458154049563845, 36.17214830292026, 36.64464218964868, 36.30789497260861, 36.40992361975561, 36.00061759527404, 36.54219975053638, 35.932682305936794]

x = range(0,len(w2v_losses))

plt.plot(x, w2v_losses, 'g', label='w2v init train')
plt.plot(x, w2v_val_losses, 'g--', label='w2v init val')

plt.plot(x, rand_losses, 'r', label='rand init train')
plt.plot(x, rand_val_losses, 'r--', label='rand init val')

plt.plot(x, non_augm_losses, 'b', label='w2v non-augm train')
plt.plot(x, non_augm_val_losses, 'b--', label='w2v non-augm val')

plt.xlabel('Epoch')
plt.xticks(x)
plt.ylabel('Loss (average)')
plt.yscale('log')

plt.legend()
plt.title('SkipGram training and validation loss')


# VOCABULARY-20

# Train losses:
w2v_v20_losses = [86.24623883374996, 41.92633906978988, 40.309091388667916, 39.3664463277506, 38.43012216874201, 38.40564282787023, 38.017696065658455, 37.80764730318559, 37.16191679877281, 37.489545856543714]

# Validation losses:
w2v_v20_val_losses = [86.93951818627995, 38.32778941650499, 37.00740947864598, 36.33538292065151, 35.95692917169741, 35.59677116483059, 35.66021194107693, 35.41597211784344, 35.62999702737998, 35.41181378420878, 35.43174001126596]

plt.show()